This one is of the course named 

# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

* Week 5 :
  * regularisarion L1 and L2. L2 implemented; Also Dropout explored 
  * checking gradient by first principal of derivative
  * Initialisation helps specially if done on basis of paper "He Initialization"; this is named for the first author of He et al., 2015

* Week 6 :
  * mini-batch gradient descend. Rem to try in power of 2 as size of batch
  * Momentum and finally ADAM (adaptive moment estimation)
